<!DOCTYPE html>

<html class="writer-html5" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>merlion.spark package — Merlion 2.0.0 documentation</title>
<link href="_static/pygments.css" rel="stylesheet" type="text/css"/>
<link href="_static/css/theme.css" rel="stylesheet" type="text/css"/>
<!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
<script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
<script src="_static/jquery.js"></script>
<script src="_static/underscore.js"></script>
<script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
<script src="_static/doctools.js"></script>
<script src="_static/sphinx_highlight.js"></script>
<script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script src="_static/js/theme.js"></script>
<link href="genindex.html" rel="index" title="Index"/>
<link href="search.html" rel="search" title="Search"/>
<link href="merlion.transform.html" rel="next" title="merlion.transform package"/>
<link href="merlion.dashboard.html" rel="prev" title="merlion.dashboard package"/>
</head>
<body class="wy-body-for-nav">
<div class="wy-grid-for-nav">
<nav class="wy-nav-side" data-toggle="wy-nav-shift">
<div class="wy-side-scroll">
<div class="wy-side-nav-search">
<a class="icon icon-home" href="index.html"> Merlion
          </a>
<div class="version">
                v2.0.0
              </div>
<div role="search">
<form action="search.html" class="wy-form" id="rtd-search-form" method="get">
<input name="q" placeholder="Search docs" type="text"/>
<input name="check_keywords" type="hidden" value="yes"/>
<input name="area" type="hidden" value="default"/>
</form>
</div>
</div><div aria-label="Navigation menu" class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation">
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="merlion.html">merlion: Time Series Intelligence</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="merlion.models.html">merlion.models package</a></li>
<li class="toctree-l2"><a class="reference internal" href="merlion.dashboard.html">merlion.dashboard package</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">merlion.spark package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#setting-up-the-spark-on-k8s-operator">Setting up the spark-on-k8s-operator</a></li>
<li class="toctree-l3"><a class="reference internal" href="#specifying-a-spark-app">Specifying a Spark App</a></li>
<li class="toctree-l3"><a class="reference internal" href="#api-documentation">API Documentation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#module-merlion.spark.dataset">merlion.spark.dataset</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#merlion.spark.dataset.TSID_COL_NAME"><code class="docutils literal notranslate"><span class="pre">TSID_COL_NAME</span></code></a></li>
<li class="toctree-l5"><a class="reference internal" href="#merlion.spark.dataset.read_dataset"><code class="docutils literal notranslate"><span class="pre">read_dataset()</span></code></a></li>
<li class="toctree-l5"><a class="reference internal" href="#merlion.spark.dataset.write_dataset"><code class="docutils literal notranslate"><span class="pre">write_dataset()</span></code></a></li>
<li class="toctree-l5"><a class="reference internal" href="#merlion.spark.dataset.create_hier_dataset"><code class="docutils literal notranslate"><span class="pre">create_hier_dataset()</span></code></a></li>
<li class="toctree-l5"><a class="reference internal" href="#merlion.spark.dataset.add_tsid_column"><code class="docutils literal notranslate"><span class="pre">add_tsid_column()</span></code></a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#module-merlion.spark.pandas_udf">merlion.spark.pandas_udf</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#merlion.spark.pandas_udf.forecast"><code class="docutils literal notranslate"><span class="pre">forecast()</span></code></a></li>
<li class="toctree-l5"><a class="reference internal" href="#merlion.spark.pandas_udf.anomaly"><code class="docutils literal notranslate"><span class="pre">anomaly()</span></code></a></li>
<li class="toctree-l5"><a class="reference internal" href="#merlion.spark.pandas_udf.reconciliation"><code class="docutils literal notranslate"><span class="pre">reconciliation()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="merlion.transform.html">merlion.transform package</a></li>
<li class="toctree-l2"><a class="reference internal" href="merlion.post_process.html">merlion.post_process package</a></li>
<li class="toctree-l2"><a class="reference internal" href="merlion.evaluate.html">merlion.evaluate package</a></li>
<li class="toctree-l2"><a class="reference internal" href="merlion.plot.html">merlion.plot package</a></li>
<li class="toctree-l2"><a class="reference internal" href="merlion.utils.html">merlion.utils package</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ts_datasets.html">ts_datasets: Easy Data Loading</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials.html">Tutorials &amp; Example Code</a></li>
</ul>
</div>
</div>
</nav>
<section class="wy-nav-content-wrap" data-toggle="wy-nav-shift"><nav aria-label="Mobile navigation menu" class="wy-nav-top">
<i class="fa fa-bars" data-toggle="wy-nav-top"></i>
<a href="index.html">Merlion</a>
</nav>
<div class="wy-nav-content">
<div class="rst-content">
<div aria-label="Page navigation" role="navigation">
<ul class="wy-breadcrumbs">
<li><a class="icon icon-home" href="index.html"></a></li>
<li class="breadcrumb-item"><a href="merlion.html">merlion: Time Series Intelligence</a></li>
<li class="breadcrumb-item active">merlion.spark package</li>
<li class="wy-breadcrumbs-aside">
<a href="_sources/merlion.spark.rst.txt" rel="nofollow"> View page source</a>
</li>
</ul>
<hr/>
</div>
<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="merlion-spark-package">
<h1>merlion.spark package<a class="headerlink" href="#merlion-spark-package" title="Permalink to this heading"></a></h1>
<p>This module implements APIs to integrate Merlion with PySpark. The expected use case is to
use distributed computing to train and run inference on multiple time series in parallel.</p>
<p>There are two ways to use the PySpark API: directly invoking the Spark apps <code class="docutils literal notranslate"><span class="pre">spark_apps/anomaly.py</span></code> and
<code class="docutils literal notranslate"><span class="pre">spark_apps/forecast.py</span></code> from the command line with either <code class="docutils literal notranslate"><span class="pre">python</span></code> or <code class="docutils literal notranslate"><span class="pre">spark-submit</span></code>,
or using the Dockerfile to serve a Spark application on a Kubernetes cluster with <code class="docutils literal notranslate"><span class="pre">spark-on-k8s</span></code>.
To understand the expected arguments for these apps, call <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">spark_apps/anomaly.py</span> <span class="pre">-h</span></code> or
<code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">spark_apps/forecast.py</span> <span class="pre">-h</span></code>.</p>
<div class="section" id="setting-up-the-spark-on-k8s-operator">
<h2>Setting up the spark-on-k8s-operator<a class="headerlink" href="#setting-up-the-spark-on-k8s-operator" title="Permalink to this heading"></a></h2>
<p>We will now cover how to serve these Spark apps using the
<a class="reference external" href="https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/">spark-on-k8s-operator</a>.
For all methods, we expect that you have installed Merlion from source by cloning our
<a class="reference external" href="https://github.com/salesforce/Merlion">git repo</a>.</p>
<p>Next, you need to create a Kubernetes cluster.
For local development, we recommend <a class="reference external" href="https://minikube.sigs.k8s.io/docs/start/">Minikube</a>.
However, you can also use Kubernetes clusters managed by major cloud providers, e.g.
<a class="reference external" href="https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/gcp.md">Google’s GKE</a> or
<a class="reference external" href="https://github.com/aws-samples/amazon-eks-apache-spark-etl-sample">Amazon’s EKS</a>. Setting up these clusters
is beyond the scope of this document, so we defer to the linked resources.</p>
<p>Once your Kubernetes cluster is set up, you need to use <a class="reference external" href="https://helm.sh/docs/intro/install/">Helm</a> to install
the <code class="docutils literal notranslate"><span class="pre">spark-on-k8s-operator</span></code>. A full quick start guide for the operator can be found
<a class="reference external" href="https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/quick-start-guide.md">here</a>,
but the key steps are to call</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>helm<span class="w"> </span>repo<span class="w"> </span>add<span class="w"> </span>spark-operator<span class="w"> </span>https://googlecloudplatform.github.io/spark-on-k8s-operator
$<span class="w"> </span>kubectl<span class="w"> </span>create<span class="w"> </span>namespace<span class="w"> </span>spark-apps
$<span class="w"> </span>helm<span class="w"> </span>install<span class="w"> </span>spark-operator<span class="w"> </span>spark-operator/spark-operator<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--namespace<span class="w"> </span>spark-operator<span class="w"> </span>--create-namespace<span class="w"> </span>--set<span class="w"> </span><span class="nv">sparkJobNamespace</span><span class="o">=</span>spark-apps
</pre></div>
</div>
<p>This will create a Kubernetes namespace <code class="docutils literal notranslate"><span class="pre">spark-apps</span></code> from which all your Spark applications will run, and it will
use Helm to install the <code class="docutils literal notranslate"><span class="pre">spark-on-k8s-operator</span></code> (which manages all running PySpark apps as Kubernetes custom
resources) in the namespace <code class="docutils literal notranslate"><span class="pre">spark-operator</span></code>.</p>
<p>Then, you can build the provided Dockerfile with <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">build</span> <span class="pre">-t</span> <span class="pre">merlion-spark</span> <span class="pre">-f</span> <span class="pre">docker/spark-on-k8s/Dockerfile</span> <span class="pre">.</span></code>
from the root directory of Merlion.
If you are using Minikube, make sure to point your shell to Minikube’s Docker daemon with
<code class="docutils literal notranslate"><span class="pre">eval</span> <span class="pre">$(minikube</span> <span class="pre">-p</span> <span class="pre">minikube</span> <span class="pre">docker-env)</span></code> before building the image.
If you are working on the cloud, you will need to publish the built Docker image to the appropriate registry, e.g.
<a class="reference external" href="https://cloud.google.com/container-registry/">Google’s gcr.io</a> or <a class="reference external" href="https://aws.amazon.com/ecr/">Amazon’s ECR</a>.</p>
<p>If you require any additional Java dependencies (e.g. to communicate with a Google GCS bucket or AWS S3 bucket),
we recommend you obtain the jars locally with a package manager like Maven,
and add a line to the Dockerfile which copies those jars to a specific path, e.g. <code class="docutils literal notranslate"><span class="pre">/opt/spark/extra-jars</span></code>.
Then, you can update the <code class="docutils literal notranslate"><span class="pre">spec.SparkConf</span></code> block of your Spark app configuration (see below) as follows:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">sparkConf</span><span class="p">:</span>
<span class="w">    </span><span class="nt">spark.driver.extraClassPath</span><span class="p">:</span><span class="w"> </span><span class="s">"local:///opt/spark/extra-jars/*"</span>
<span class="w">    </span><span class="nt">spark.executor.extraClassPath</span><span class="p">:</span><span class="w"> </span><span class="s">"local:///opt/spark/extra-jars/*"</span>
</pre></div>
</div>
</div>
<div class="section" id="specifying-a-spark-app">
<h2>Specifying a Spark App<a class="headerlink" href="#specifying-a-spark-app" title="Permalink to this heading"></a></h2>
<p>Once your cluster is set up, you can submit a YAML file specifying your spark application as a Kubernetes custom
resource. We provide templates for both forecasting and anomaly detection in <code class="docutils literal notranslate"><span class="pre">k8s-spec/forecast.yml</span></code> and
<code class="docutils literal notranslate"><span class="pre">k8s-spec/anomaly.yml</span></code> respectively. Both of these use the <code class="docutils literal notranslate"><span class="pre">walmart_mini.csv</span></code> dataset,
which contains the weekly sales of 10 different products at 2 different stores.</p>
<p>You can change the Docker image used by changing the <code class="docutils literal notranslate"><span class="pre">spec.image</span></code> in the YAML file. You can modify the amount of
computational resources allocated to the Spark driver and executor by modifying <code class="docutils literal notranslate"><span class="pre">spec.driver</span></code> and <code class="docutils literal notranslate"><span class="pre">spec.executor</span></code>
respectively. The arguments to the main application file (<code class="docutils literal notranslate"><span class="pre">spark_apps/anomaly.py</span></code> or <code class="docutils literal notranslate"><span class="pre">spark_apps/forecast.py</span></code>)
are specified as a YAML list under <code class="docutils literal notranslate"><span class="pre">spec.arguments</span></code>. These should be modified according to your use case.
By adding the appropriate Java dependencies and modifying the <code class="docutils literal notranslate"><span class="pre">spec.sparkConf</span></code>, you can directly read and write files
on cloud storage buckets. While this topic is beyond the scope of this document, we refer an interested reader to
<a class="reference external" href="https://spark.apache.org/docs/latest/configuration.html#custom-hadoophive-configuration">Spark’s Hadoop config</a>,
<a class="reference external" href="https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html">Hadoop’s AWS S3 connector</a>, and the
<a class="reference external" href="https://cloud.google.com/dataproc/docs/tutorials/gcs-connector-spark-tutorial">GCS connector</a> for more information.</p>
<p>More detailed information about specifying a Spark application can be found in the <code class="docutils literal notranslate"><span class="pre">spark-on-k8s-operator</span></code>’s detailed
<a class="reference external" href="https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/api-docs.md">API documentation</a>.</p>
</div>
<div class="section" id="api-documentation">
<h2>API Documentation<a class="headerlink" href="#api-documentation" title="Permalink to this heading"></a></h2>
<p>The API documentation of Merlion’s PySpark connectors (<code class="docutils literal notranslate"><span class="pre">merlion.spark</span></code>) is below.</p>
<span class="target" id="module-merlion.spark"></span><table class="autosummary longtable docutils align-default">
<colgroup>
<col style="width: 10%"/>
<col style="width: 90%"/>
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#module-merlion.spark.dataset" title="merlion.spark.dataset"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dataset</span></code></a></p></td>
<td><p>Utils for reading &amp; writing pyspark datasets.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#module-merlion.spark.pandas_udf" title="merlion.spark.pandas_udf"><code class="xref py py-obj docutils literal notranslate"><span class="pre">pandas_udf</span></code></a></p></td>
<td><p>Pyspark pandas UDFs for Merlion functions.</p></td>
</tr>
</tbody>
</table>
<div class="section" id="module-merlion.spark.dataset">
<span id="merlion-spark-dataset"></span><h3>merlion.spark.dataset<a class="headerlink" href="#module-merlion.spark.dataset" title="Permalink to this heading"></a></h3>
<p>Utils for reading &amp; writing pyspark datasets.</p>
<dl class="py data">
<dt class="sig sig-object py" id="merlion.spark.dataset.TSID_COL_NAME">
<span class="sig-prename descclassname"><span class="pre">merlion.spark.dataset.</span></span><span class="sig-name descname"><span class="pre">TSID_COL_NAME</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'__ts_id'</span></em><a class="headerlink" href="#merlion.spark.dataset.TSID_COL_NAME" title="Permalink to this definition"></a></dt>
<dd><p>Many functions in this module rely on having a column named <a class="reference internal" href="#merlion.spark.dataset.TSID_COL_NAME" title="merlion.spark.dataset.TSID_COL_NAME"><code class="xref any py py-data docutils literal notranslate"><span class="pre">TSID_COL_NAME</span></code></a> being in the dataset.
This column can be added manually using <a class="reference internal" href="#merlion.spark.dataset.add_tsid_column" title="merlion.spark.dataset.add_tsid_column"><code class="xref any py py-func docutils literal notranslate"><span class="pre">add_tsid_column</span></code></a>, and its addition is handled automatically by <a class="reference internal" href="#merlion.spark.dataset.read_dataset" title="merlion.spark.dataset.read_dataset"><code class="xref any py py-func docutils literal notranslate"><span class="pre">read_dataset</span></code></a>.</p>
</dd></dl>
<dl class="py function">
<dt class="sig sig-object py" id="merlion.spark.dataset.read_dataset">
<span class="sig-prename descclassname"><span class="pre">merlion.spark.dataset.</span></span><span class="sig-name descname"><span class="pre">read_dataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">spark</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">file_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'csv'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_col</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index_cols</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data_cols</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#merlion.spark.dataset.read_dataset" title="Permalink to this definition"></a></dt>
<dd><p>Reads a time series dataset as a pyspark Dataframe.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>spark</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">SparkSession</span></code>) – The current SparkSession.</p></li>
<li><p><strong>path</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The path at which the dataset is stored.</p></li>
<li><p><strong>file_format</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The file format the dataset is stored in.</p></li>
<li><p><strong>time_col</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – The name of the column which specifies timestamp. If <code class="docutils literal notranslate"><span class="pre">None</span></code> is provided, it is assumed to be the
first column which is not an index column or pre-specified data column.</p></li>
<li><p><strong>index_cols</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]]) – The columns used to index the various time series in the dataset. If <code class="docutils literal notranslate"><span class="pre">None</span></code> is provided, we
assume the entire dataset is just a single time series.</p></li>
<li><p><strong>data_cols</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]]) – The columns we will use for downstream time series tasks. If <code class="docutils literal notranslate"><span class="pre">None</span></code> is provided, we use all
columns that are not a time or index column.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A pyspark dataframe with columns <code class="docutils literal notranslate"><span class="pre">[time_col,</span> <span class="pre">*index_cols,</span> <span class="pre">*data_cols,</span> <span class="pre">TSID_COL_NAME]</span></code> (in that order).</p>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt class="sig sig-object py" id="merlion.spark.dataset.write_dataset">
<span class="sig-prename descclassname"><span class="pre">merlion.spark.dataset.</span></span><span class="sig-name descname"><span class="pre">write_dataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_col</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">file_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'csv'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#merlion.spark.dataset.write_dataset" title="Permalink to this definition"></a></dt>
<dd><p>Writes the dataset at the specified path.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>df</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code>) – The dataframe to save. The dataframe must have a column <a class="reference internal" href="#merlion.spark.dataset.TSID_COL_NAME" title="merlion.spark.dataset.TSID_COL_NAME"><code class="xref any py py-data docutils literal notranslate"><span class="pre">TSID_COL_NAME</span></code></a>
indexing the time series in the dataset (this column is automatically added by <a class="reference internal" href="#merlion.spark.dataset.read_dataset" title="merlion.spark.dataset.read_dataset"><code class="xref any py py-func docutils literal notranslate"><span class="pre">read_dataset</span></code></a>).</p></li>
<li><p><strong>time_col</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The name of the column which specifies timestamp.</p></li>
<li><p><strong>path</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The path to save the dataset at.</p></li>
<li><p><strong>file_format</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The file format in which to save the dataset.</p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt class="sig sig-object py" id="merlion.spark.dataset.create_hier_dataset">
<span class="sig-prename descclassname"><span class="pre">merlion.spark.dataset.</span></span><span class="sig-name descname"><span class="pre">create_hier_dataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">spark</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_col</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index_cols</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">agg_dict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#merlion.spark.dataset.create_hier_dataset" title="Permalink to this definition"></a></dt>
<dd><p>Aggregates the time series in the dataset &amp; appends them to the original dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>spark</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">SparkSession</span></code>) – The current SparkSession.</p></li>
<li><p><strong>df</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code>) – A pyspark dataframe containing all the data. The dataframe must have a column <a class="reference internal" href="#merlion.spark.dataset.TSID_COL_NAME" title="merlion.spark.dataset.TSID_COL_NAME"><code class="xref any py py-data docutils literal notranslate"><span class="pre">TSID_COL_NAME</span></code></a>
indexing the time series in the dataset (this column is automatically added by <a class="reference internal" href="#merlion.spark.dataset.read_dataset" title="merlion.spark.dataset.read_dataset"><code class="xref any py py-func docutils literal notranslate"><span class="pre">read_dataset</span></code></a>).</p></li>
<li><p><strong>time_col</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – The name of the column which specifies timestamp. If <code class="docutils literal notranslate"><span class="pre">None</span></code> is provided, it is assumed to be the
first column which is not an index column or pre-specified data column.</p></li>
<li><p><strong>index_cols</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]]) – The columns used to index the various time series in the dataset. If <code class="docutils literal notranslate"><span class="pre">None</span></code> is provided, we
assume the entire dataset is just a single time series. These columns define the levels of the hierarchy.
For example, if each time series represents sales and we have <code class="docutils literal notranslate"><span class="pre">index_cols</span> <span class="pre">=</span> <span class="pre">["store",</span> <span class="pre">"item"]</span></code>, we will
first aggregate sales for all items sold at a particular store; then we will aggregate sales for all items at
all stores.</p></li>
<li><p><strong>agg_dict</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>]) – A dictionary used to specify how different data columns should be aggregated. If a data column
is not in the dict, we aggregate using sum by default.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The dataset with additional time series corresponding to each level of the hierarchy, as well as a
matrix specifying how the hierarchy is constructed.</p>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt class="sig sig-object py" id="merlion.spark.dataset.add_tsid_column">
<span class="sig-prename descclassname"><span class="pre">merlion.spark.dataset.</span></span><span class="sig-name descname"><span class="pre">add_tsid_column</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">spark</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">df</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index_cols</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#merlion.spark.dataset.add_tsid_column" title="Permalink to this definition"></a></dt>
<dd><p>Adds the column <a class="reference internal" href="#merlion.spark.dataset.TSID_COL_NAME" title="merlion.spark.dataset.TSID_COL_NAME"><code class="xref any py py-data docutils literal notranslate"><span class="pre">TSID_COL_NAME</span></code></a> to the dataframe, which assigns an integer ID to each time series in the dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>spark</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">SparkSession</span></code>) – The current SparkSession.</p></li>
<li><p><strong>df</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code>) – A pyspark dataframe containing all the data.</p></li>
<li><p><strong>index_cols</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – The columns used to index the various time series in the dataset.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The pyspark dataframe with an additional column <a class="reference internal" href="#merlion.spark.dataset.TSID_COL_NAME" title="merlion.spark.dataset.TSID_COL_NAME"><code class="xref any py py-data docutils literal notranslate"><span class="pre">TSID_COL_NAME</span></code></a> added as the last column.</p>
</dd>
</dl>
</dd></dl>
</div>
<div class="section" id="module-merlion.spark.pandas_udf">
<span id="merlion-spark-pandas-udf"></span><h3>merlion.spark.pandas_udf<a class="headerlink" href="#module-merlion.spark.pandas_udf" title="Permalink to this heading"></a></h3>
<p>Pyspark pandas UDFs for Merlion functions.
This module contains pandas UDFs that can be called via <code class="docutils literal notranslate"><span class="pre">pyspark.sql.DataFrame.applyInPandas</span></code> to run Merlion
forecasting, anomaly detection, and time series reconciliation in parallel.</p>
<dl class="py function">
<dt class="sig sig-object py" id="merlion.spark.pandas_udf.forecast">
<span class="sig-prename descclassname"><span class="pre">merlion.spark.pandas_udf.</span></span><span class="sig-name descname"><span class="pre">forecast</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pdf</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index_cols</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_col</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_col</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_stamps</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predict_on_train</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">agg_dict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#merlion.spark.pandas_udf.forecast" title="Permalink to this definition"></a></dt>
<dd><p>Pyspark pandas UDF for performing forecasting.
Should be called on a pyspark dataframe grouped by time series ID, i.e. by <code class="docutils literal notranslate"><span class="pre">index_cols</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pdf</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code>) – The <code class="docutils literal notranslate"><span class="pre">pandas.DataFrame</span></code> containing the training data. Should be a single time series.</p></li>
<li><p><strong>index_cols</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – The list of column names used to index all the time series in the dataset. Not used for modeling.</p></li>
<li><p><strong>time_col</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The name of the column containing the timestamps.</p></li>
<li><p><strong>target_col</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The name of the column whose value we wish to forecast.</p></li>
<li><p><strong>time_stamps</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]]) – The timestamps at which we would like to obtain a forecast.</p></li>
<li><p><strong>model</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<a class="reference internal" href="merlion.models.forecast.html#merlion.models.forecast.base.ForecasterBase" title="merlion.models.forecast.base.ForecasterBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">ForecasterBase</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code>]) – The model (or model <code class="docutils literal notranslate"><span class="pre">dict</span></code>) we are using to obtain a forecast.</p></li>
<li><p><strong>predict_on_train</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – Whether to return the model’s prediction on the training data.</p></li>
<li><p><strong>agg_dict</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code>]) – A dictionary used to specify how different data columns should be aggregated. If a non-target
data column is not in agg_dict, we do not model it for aggregated time series.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A <code class="docutils literal notranslate"><span class="pre">pandas.DataFrame</span></code> with the forecast &amp; its standard error (NaN if the model doesn’t have error bars).
Columns are <code class="docutils literal notranslate"><span class="pre">[*index_cols,</span> <span class="pre">time_col,</span> <span class="pre">target_col,</span> <span class="pre">target_col</span> <span class="pre">+</span> <span class="pre">"_err"]</span></code>.</p>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt class="sig sig-object py" id="merlion.spark.pandas_udf.anomaly">
<span class="sig-prename descclassname"><span class="pre">merlion.spark.pandas_udf.</span></span><span class="sig-name descname"><span class="pre">anomaly</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pdf</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index_cols</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_col</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_test_split</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">predict_on_train</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#merlion.spark.pandas_udf.anomaly" title="Permalink to this definition"></a></dt>
<dd><p>Pyspark pandas UDF for performing anomaly detection.
Should be called on a pyspark dataframe grouped by time series ID, i.e. by <code class="docutils literal notranslate"><span class="pre">index_cols</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pdf</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code>) – The <code class="docutils literal notranslate"><span class="pre">pandas.DataFrame</span></code> containing the training and testing data. Should be a single time series.</p></li>
<li><p><strong>index_cols</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – The list of column names used to index all the time series in the dataset. Not used for modeling.</p></li>
<li><p><strong>time_col</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The name of the column containing the timestamps.</p></li>
<li><p><strong>train_test_split</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – The time at which the testing data starts.</p></li>
<li><p><strong>model</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<a class="reference internal" href="merlion.models.anomaly.html#merlion.models.anomaly.base.DetectorBase" title="merlion.models.anomaly.base.DetectorBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">DetectorBase</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code>]) – The model (or model <code class="docutils literal notranslate"><span class="pre">dict</span></code>) we are using to predict anomaly scores.</p></li>
<li><p><strong>predict_on_train</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – Whether to return the model’s prediction on the training data.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A <code class="docutils literal notranslate"><span class="pre">pandas.DataFrame</span></code> with the anomaly scores on the test data.
Columns are <code class="docutils literal notranslate"><span class="pre">[*index_cols,</span> <span class="pre">time_col,</span> <span class="pre">"anom_score"]</span></code>.</p>
</dd>
</dl>
</dd></dl>
<dl class="py function">
<dt class="sig sig-object py" id="merlion.spark.pandas_udf.reconciliation">
<span class="sig-prename descclassname"><span class="pre">merlion.spark.pandas_udf.</span></span><span class="sig-name descname"><span class="pre">reconciliation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pdf</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hier_matrix</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_col</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#merlion.spark.pandas_udf.reconciliation" title="Permalink to this definition"></a></dt>
<dd><p>Pyspark pandas UDF for computing the minimum-trace hierarchical time series reconciliation, as described by
<a class="reference external" href="https://robjhyndman.com/papers/mint.pdf">Wickramasuriya et al. 2018</a>.
Should be called on a pyspark dataframe grouped by timestamp. Pyspark implementation of
<a class="reference internal" href="merlion.utils.html#merlion.utils.hts.minT_reconciliation" title="merlion.utils.hts.minT_reconciliation"><code class="xref any py py-func docutils literal notranslate"><span class="pre">merlion.utils.hts.minT_reconciliation</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pdf</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">DataFrame</span></code>) – A <code class="docutils literal notranslate"><span class="pre">pandas.DataFrame</span></code> containing forecasted values &amp; standard errors from <code class="docutils literal notranslate"><span class="pre">m</span></code> time series at a single
timestamp. Each time series should be indexed by <a class="reference internal" href="#merlion.spark.dataset.TSID_COL_NAME" title="merlion.spark.dataset.TSID_COL_NAME"><code class="xref any py py-data docutils literal notranslate"><span class="pre">TSID_COL_NAME</span></code></a>.
The first <code class="docutils literal notranslate"><span class="pre">n</span></code> time series (in order of ID) orrespond to leaves of the hierarchy, while the remaining <code class="docutils literal notranslate"><span class="pre">m</span> <span class="pre">-</span> <span class="pre">n</span></code>
are weighted sums of the first <code class="docutils literal notranslate"><span class="pre">n</span></code>.
This dataframe can be produced by calling <a class="reference internal" href="#merlion.spark.pandas_udf.forecast" title="merlion.spark.pandas_udf.forecast"><code class="xref any py py-func docutils literal notranslate"><span class="pre">forecast</span></code></a> on the dataframe produced by
<a class="reference internal" href="#merlion.spark.dataset.create_hier_dataset" title="merlion.spark.dataset.create_hier_dataset"><code class="xref any py py-func docutils literal notranslate"><span class="pre">merlion.spark.dataset.create_hier_dataset</span></code></a>.</p></li>
<li><p><strong>hier_matrix</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>) – A <code class="docutils literal notranslate"><span class="pre">m</span></code>-by-<code class="docutils literal notranslate"><span class="pre">n</span></code> matrix describing how the hierarchy is aggregated. The value of the <code class="docutils literal notranslate"><span class="pre">k</span></code>-th
time series is <code class="docutils literal notranslate"><span class="pre">np.dot(hier_matrix[k],</span> <span class="pre">pdf[:n])</span></code>. This matrix can be produced by
<a class="reference internal" href="#merlion.spark.dataset.create_hier_dataset" title="merlion.spark.dataset.create_hier_dataset"><code class="xref any py py-func docutils literal notranslate"><span class="pre">merlion.spark.dataset.create_hier_dataset</span></code></a>.</p></li>
<li><p><strong>target_col</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The name of the column whose value we wish to forecast.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="docutils literal notranslate"><span class="pre">pandas.DataFrame</span></code> which replaces the original forecasts &amp; errors with reconciled forecasts &amp; errors.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Time series series reconciliation is skipped if the given timestamp has missing values for any of the
time series. This can happen for training timestamps if the training time series has missing data and
<a class="reference internal" href="#merlion.spark.pandas_udf.forecast" title="merlion.spark.pandas_udf.forecast"><code class="xref any py py-func docutils literal notranslate"><span class="pre">forecast</span></code></a> is called with <code class="docutils literal notranslate"><span class="pre">predict_on_train=true</span></code>.</p>
</div>
</dd></dl>
</div>
</div>
</div>
</div>
</div>
<footer><div aria-label="Footer" class="rst-footer-buttons" role="navigation">
<a accesskey="p" class="btn btn-neutral float-left" href="merlion.dashboard.html" rel="prev" title="merlion.dashboard package"><span aria-hidden="true" class="fa fa-arrow-circle-left"></span> Previous</a>
<a accesskey="n" class="btn btn-neutral float-right" href="merlion.transform.html" rel="next" title="merlion.transform package">Next <span aria-hidden="true" class="fa fa-arrow-circle-right"></span></a>
</div>
<hr/>
<div role="contentinfo">
<p>© Copyright 2021, salesforce.com, inc..</p>
</div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
</div>
</div>
</section>
</div>
<div aria-label="versions" class="rst-versions" data-toggle="rst-versions" role="note">
<span class="rst-current-version" data-toggle="rst-current-version">
<span class="fa fa-book"> Versions</span>
      v2.0.0
      <span class="fa fa-caret-down"></span>
</span>
<div class="rst-other-versions">
<dl><dt>Versions</dt><dd><a href="../latest/index.html">latest</a></dd><dd><a href="../v2.0.2/index.html">v2.0.2</a></dd><dd><a href="../v2.0.1/index.html">v2.0.1</a></dd><dd><strong><a href="../v2.0.0/index.html">v2.0.0</a></strong></dd><dd><a href="../v1.3.1/index.html">v1.3.1</a></dd><dd><a href="../v1.3.0/index.html">v1.3.0</a></dd><dd><a href="../v1.2.5/index.html">v1.2.5</a></dd><dd><a href="../v1.2.4/index.html">v1.2.4</a></dd><dd><a href="../v1.2.3/index.html">v1.2.3</a></dd><dd><a href="../v1.2.2/index.html">v1.2.2</a></dd><dd><a href="../v1.2.1/index.html">v1.2.1</a></dd><dd><a href="../v1.2.0/index.html">v1.2.0</a></dd><dd><a href="../v1.1.3/index.html">v1.1.3</a></dd><dd><a href="../v1.1.2/index.html">v1.1.2</a></dd><dd><a href="../v1.1.1/index.html">v1.1.1</a></dd><dd><a href="../v1.1.0/index.html">v1.1.0</a></dd><dd><a href="../v1.0.2/index.html">v1.0.2</a></dd><dd><a href="../v1.0.1/index.html">v1.0.1</a></dd><dd><a href="../v1.0.0/index.html">v1.0.0</a></dd></dl>
</div>
</div>
<script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
</body>
</html>